---
title: "Bayesiläinen bootstrap"
author: "Johannes Rajala"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(LaplacesDemon)
library(ggplot2)
library(tidyverse)
```

### Johdanto

Perinteinen bootstrap on uudelleenotantamenetelmä, jossa alkuperäisestä $n$:n pituisesta otoksesta tehdään $n$:n pituinen uudelleenotanta palauttamalla. Näin saatua uutta otosta kutsutaan bootstrap-otokseksi, ja niitä käytetään usein estimoimaan alkuperäisestä datasta lasketun estimaatin jakaumaa. 

### Esimerkki

Lasketaan esimerkkidatan keskiarvon boostrap-jakauma.

```{r}
data = c(1,2,4,4,5,8,2,2,9)

bootstrap = function(data, f, n){
  boot_results = c() # Vektori bootstrap tuloksia varten
  
  for(i in (1:n)){
    boot_results[i] = f(sample(data, length(data), replace = TRUE)) # Iteroidaan n kertaa ja lasketaan joka kerta funktion arvo bootstrap otokselle 
  }
  
  return(boot_results) # Palautetaan bootstrap otoksille lasketut funktion arvot 
}
```

```{r}
results = as_tibble(bootstrap(data, {function(x) mean(x)}, 100000))
cat("Bootstrap-keskiarvojen varianssi:",var(results))
```

```{r echo=FALSE}
ggplot(data = results, aes(x=value)) +
  geom_histogram(bins = 40, col = "black", fill = "white") +
  geom_vline(xintercept = c(quantile(results$value, c(0.025, 0.975))), col = "red", linewidth = 0.6) +
   ggtitle("Keskiarvon bootstrap-jakauma 2.5% ja 97.5% kvantiileilla")
  
```

### Bootstrap painoilla

<<<<<<< HEAD
Bootstrapin voidaan ajatella toimivan myös niin, että jokaiselle alkuperäisen otoksen havainnolle $y_i$ arvotaan kokonaislukupaino $w_i$ siten, että $\sum_{i=1}^n w_i = n$, tai vastaavasti $\sum_{i=1}^n \frac{1}{n}w_i = 1$. Boootstrap otoksen $B^{(k)}$ painot noudattavat siis multinomijakaumaa, $W^{(k)} \sim \text{Multinomi}(1, n, (\frac{1}{n},\ldots,\frac{1}{n}))$.

### Bayesiläinen bootstrap

Painojen jakaumaksi voidaan valita disktreetin multinomijakauman sijaan jatkuva Dirichlet-jakauma, $\frac{1}{n}W^{(k)} \sim \text{Dirichlet}( 1, (1,\ldots,1))$, jolle pätee $\sum_{i=1}^nw_i = 1$, ja vastaavasti $\sum_{i=1}^nnw_i = n$. Dirichlet-jakaumaa voidaan käyttää epäinformatiivisena priorina alkuperäiselle datalle tehdylle estimaatille, ja näin saadaan parametrin estimaatin posteriorijakauma. 
=======
Bootstrapin voidaan ajatella toimivan myös niin, että jokaiselle alkuperäisen otoksen havainnolle $y_i$ arvotaan kokonaislukupaino $w_i$ siten, että $\sum_{i=1}^n w_i = n$. Boootstrap otoksen $B^{(k)}$ painot noudattavat siis multinomijakaumaa, $W^{(K)} \sim \text{Multinomi}(n, (\frac{1}{n},\ldots,\frac{1}{n}))$, jonka pistetodennäköisyysfunktio on yleisessä muodossa:

\[
f(w_1 \ldots w_k ; n , p_1 \ldots p_k) = p( W_1 = w_1, \ldots, W_k = w_k)= \begin{cases} 
\frac{n!} {w_1! \ldots w_k!}p_1^{w_1} \ldots p_k^{w_k} & , \text{ kun } \sum_{i=1}^{k} w_i = n \\
0 & , \text{ muuten.}
\end{cases}
\]

Tämä voidaan esittää myös gammafunktion avulla: 

\[
f(w_1 \ldots w_k ; p_1 \ldots p_k ) = \begin{cases} 
\frac{ \Gamma( \sum_{i=1}^k w_i + 1)}{ \prod_{i=1}^k \Gamma(w_i+1)} \prod_{i=1}^k p_i^{k_i} & , \text{ kun } \sum_{i=1}^{k} w_i = 1 \\
0 & , \text{ muuten.}
\end{cases}
\]

### Bayesiläinen bootstrap

Kokonaislukujen sijaan painoiksi voidaan valita positiiviset reaalipainot, jotka summautuvat yhteen. Painojen jakaumaksi voidaan siis valita disktreetin multinomijakauman sijaan jatkuva Dirichlet-jakauma, $\frac{1}{n}W^{(K)} \sim \text{Dirichlet}( 1, (\frac{1}{n}\ldots\frac{1}{n}))$, jolle pätee $\sum_{i=1}^nw_i = n$.

Dirichlet-jakauman tiheysfunktio voidaan esittää muodossa:

\[
f(w_1 \ldots w_k ; \alpha_1 \ldots \alpha_k) = \begin{cases} 
\frac{ \Gamma( \sum_{i=1}^k \alpha_i)}{ \prod_{i=1}^k \Gamma( \alpha_i)} \prod_{i=1}^k w_i^{ \alpha_i - 1} & , \text{ kun } \sum_{i=1}^{k} w_i = 1 \\
0 & , \text{ muuten.}
\end{cases}
\]

Dirichlet-jakaumaa voidaan käyttää epäinformatiivisena priorina alkuperäiselle datalle tehdylle estimaatille, ja näin saadaan parametrin estimaatin posteriorijakauma. 
>>>>>>> e5694c599e276dfb74ed2690ed201b0e05c3e528

### Esimerkki

Tehdään sama esimerkki keskiarvolle kuin alussa, mutta bayselaisella bootstrapilla.

```{r}
data = c(1,2,4,4,5,8,2,2,9) # sama data kuin ensimmäisessä esimerkissä

bootstrap_bayes = function(data_, f, n = 10000, alpha = rep(1, length(data_))){
  boot_results = c() # Vektori bootstrap tuloskia varten
  len = length(data_) # datan pituus

  for(i in (1:n)){
    weights = rdirichlet(1, alpha)*len # lasketaan painot dirichlet jakaumasta
    boot_results[i] = f(data_*weights) # kerrotaan data painoilla
  }

  return(boot_results) # Palautetaan bootstrap otoksille lasketut funktion arvot 
}
```

```{r}
results_bayes = as_tibble(bootstrap_bayes(data, {function(x) mean(x)}, 100000, alpha = rep(1, length(data))))
cat("Bayesiläisten bootstrap-keskiarvojen varianssi:",var(results_bayes))
```

```{r echo=FALSE}
ggplot(data = results_bayes, aes(x=value)) +
  geom_histogram(bins = 40, col = "black", fill = "white") +
  geom_vline(xintercept = c(quantile(results_bayes$value, c(0.025, 0.975))), col = "red", linewidth = 0.6) +
   ggtitle("Keskiarvon posteriori-jakauma 2.5% ja 97.5% kvantiileilla")
```

Saatu posteriorijakauma on paljon sileämpi kuin perinteinen keskiarvon boostrap-jakauma ja varianssin estimaatti on pienempi.

